{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main_GMP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WLSoQkq0cD8e"},"source":["As a baseline for experiments with dense-to-sparse pruning algorithms, an implementation of global magnitude pruning is provided below (\"prune_gmp\"). "]},{"cell_type":"code","metadata":{"id":"uEN_sVm8pWMe"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","#from torch.nn.utils import prune\n","from models import VGG, resnet\n","from data_utils import get_dataloader\n","from prune_utils import _count_unmasked_weights, _count_all_weights, prune_weights_reparam, prune_weights_erk\n","\n","def train(net, loader, optimizer, criterion, epoch, verbose=False):\n","    net.train()\n","    train_loss, correct, total = 0, 0, 0\n","    for batch_idx, (inputs, targets) in enumerate(loader):\n","        inputs, targets = inputs.cuda(), targets.cuda()\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","    if verbose:\n","        print(\"epoch, learning rate\", epoch, optimizer.param_groups[0]['lr'], \"training accuracy\", 100*correct/total)\n","\n","def test(net, loader, verbose=False):\n","    criterion = nn.CrossEntropyLoss()\n","    net.eval()\n","    test_loss, correct, total = 0, 0, 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(loader):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","    acc = 100. * correct / total\n","    if verbose:\n","      print(\"testing accuracy is\", acc)\n","    return acc\n","\n","\n","def prune_gmp(dataset=\"cifar100\", network=\"resnet\", num_epochs=240, sparsity=0.05, k=80, l=100, verbose=False):\n","    if network==\"resnet\":\n","        net = resnet(depth=32, dataset=dataset).cuda()\n","    if network==\"vgg\":\n","        net = VGG(depth=19, dataset=dataset, batchnorm=True).cuda()\n","    trainloader, testloader = get_dataloader(dataset, 128, 128, 2, root='/content')\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,180], gamma=0.1)\n","    prune_weights_reparam(net)\n","    alpha = 1-sparsity**(1/float(l))\n","    if verbose:\n","      print(net)\n","      print(\"pruning amount is\", alpha)\n","    for epoch in range(num_epochs):\n","        train(net, trainloader, optimizer, criterion, epoch, verbose)\n","        test(net, testloader, verbose)\n","        unmasked_weights = _count_unmasked_weights(net)\n","        sparsity_ratio = torch.sum(unmasked_weights)/_count_all_weights(net)\n","        scheduler.step()\n","        if verbose:\n","          print(\"unmasked weights\", unmasked_weights, \"sparsity ratio\", sparsity_ratio)\n","        if epoch>k and epoch<=k+l:\n","          prune_weights_global(net,alpha)\n","\n","prune_gmp(verbose=True)"],"execution_count":null,"outputs":[]}]}